# Water Quality Data Service

A complete data pipeline that transforms raw water quality CSV data into an interactive web service, featuring data cleaning, REST API, and visualization dashboard.

## ğŸš€ Project Overview

This project implements a full-stack data pipeline:
**CSV â†’ Data Cleaning â†’ NoSQL Database â†’ REST API â†’ Interactive Dashboard**

### Key Features
- **ETL Pipeline**: Load, clean (z-score outlier removal), and store water quality data
- **REST API**: Flask-based API with filtering, statistics, and outlier detection
- **Interactive Dashboard**: Streamlit client with real-time visualizations
- **Data Cleaning**: Z-score method for outlier detection and removal
- **NoSQL Storage**: MongoDB/mongomock for data persistence

## ğŸ“‹ Requirements

### Dependencies
- Python 3.9+
- Flask 3.1.2
- Streamlit 1.50.0
- pandas 2.3.3
- plotly 6.0.0+
- requests 2.32.5
- pymongo 4.9.2
- mongomock 4.1.2
- scipy 1.13.1

## ğŸ› ï¸ Installation & Setup

### 1. Clone Repository
```bash
git clone https://github.com/Marcosfpai/internshipReady.git
cd internshipReady
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Data Setup
The project uses `data/raw.csv` which contains water quality measurements with columns:
- Latitude, Longitude
- Temperature (c)
- Salinity (ppt)  
- ODO mg/L
- Date m/d/y, Time hh:mm:ss

**Note**: You can use any CSV file by setting the `DATA_FILE` environment variable (default: `raw.csv`)

## ğŸ”„ Running the Application

### Step 1: Start the Flask API
```bash
cd api
python app.py
```
The API will be available at `http://localhost:5000`

**Using a different data file:**
```bash
# PowerShell
$env:DATA_FILE="2021-dec16.csv"; python app.py

# Bash/Linux/Mac
DATA_FILE="2021-dec16.csv" python app.py
```

### Step 2: Start the Streamlit Dashboard
In a new terminal:
```bash
cd client
streamlit run app.py
```
The dashboard will open at `http://localhost:8501`

## ğŸ“Š API Documentation

### Base URL: `http://localhost:5000/api`

#### Health Check
```
GET /api/health
```
**Response:**
```json
{"status": "ok"}
```

#### Get Observations
```
GET /api/observations?[filters]
```
**Query Parameters:**
- `start`, `end` - ISO timestamp range
- `min_temp`, `max_temp` - Temperature range (Â°C)
- `min_sal`, `max_sal` - Salinity range (ppt)
- `min_odo`, `max_odo` - ODO range (mg/L)
- `limit` - Max records (default: 100, max: 1000)
- `skip` - Pagination offset

**Example:**
```
GET /api/observations?min_temp=25&max_temp=30&limit=100
```

**Response:**
```json
{
  "count": 503,
  "returned": 100,
  "items": [
    {
      "timestamp": "2022-10-07T11:02:04",
      "latitude": 25.91273,
      "longitude": -80.13782,
      "temperature": 27.2,
      "salinity": 35.1,
      "odo": 6.7
    }
  ]
}
```

#### Get Statistics
```
GET /api/stats
```
**Response:**
```json
{
  "temperature": {
    "count": 1500,
    "mean": 27.85,
    "std": 1.23,
    "min": 25.1,
    "25%": 26.8,
    "50%": 27.9,
    "75%": 28.7,
    "max": 30.2
  }
}
```

#### Get Outliers
```
GET /api/outliers?field=temperature&method=iqr&k=1.5
```
**Query Parameters:**
- `field` - Field to analyze (temperature, salinity, odo)
- `method` - Detection method (iqr, zscore)
- `k` - Threshold multiplier

**Response:**
```json
{
  "method": "iqr",
  "field": "temperature", 
  "k": 1.5,
  "outlier_count": 23,
  "outliers": [...]
}
```

#### Get Summary
```
GET /api/summary
```
**Response:**
```json
{
  "original_rows": 1772,
  "rows_removed": 89,
  "rows_remaining": 1683,
  "cleaning_method": "z-score with threshold 3.0"
}
```

## ğŸ–¥ï¸ Dashboard Features

### Filter Controls (Sidebar)
- **Date Range**: Start/end date pickers
- **Temperature**: Min/max sliders (Â°C)
- **Salinity**: Min/max sliders (ppt)
- **ODO**: Min/max sliders (mg/L)
- **Pagination**: Records per page, skip offset

### Data Table Tab
- Filtered observations display
- Real-time record counts
- CSV download functionality

### Visualizations Tab
- **Temperature Trends**: Line chart over time
- **Salinity Distribution**: Histogram with bins
- **Scatter Plots**: Temperature vs ODO relationships
- **Geographic Map**: Interactive map with measurement locations

### Statistics Tab
- **API Statistics**: Mean, std dev, percentiles from `/api/stats`
- **Filter Summary**: Current dataset descriptive statistics
- **Metric Cards**: Key performance indicators

### Outliers Tab
- **Interactive Detection**: Choose field, method, and threshold
- **Visual Outliers**: Scatter plot highlighting anomalies
- **Outlier Table**: Detailed outlier records

## ğŸ§¹ Data Cleaning Process

### Z-Score Method
1. **Load Raw Data**: Read CSV with ~1,772 observations
2. **Column Mapping**: Normalize column names
3. **Timestamp Creation**: Generate from date/time fields
4. **Missing Values**: Remove incomplete records
5. **Z-Score Calculation**: Compute for temperature, salinity, ODO
6. **Outlier Removal**: Drop records where |z| > 3.0
7. **Database Storage**: Insert cleaned data into MongoDB

### Results
- **Original Records**: 1,772
- **Outliers Removed**: ~89 (5.0%)
- **Clean Records**: 1,683 (95.0%)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw CSV   â”‚    â”‚  Flask API   â”‚    â”‚  Streamlit  â”‚
â”‚    Data     â”‚â”€â”€â”€â–¶â”‚   (ETL +     â”‚â—€â”€â”€â”€â”‚  Dashboard  â”‚
â”‚             â”‚    â”‚  Endpoints)  â”‚    â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  MongoDB/    â”‚
                   â”‚  mongomock   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
internshipReady/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ app.py              # Flask REST API
â”œâ”€â”€ client/
â”‚   â””â”€â”€ app.py              # Streamlit dashboard
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw.csv             # Raw water quality data
â”œâ”€â”€ datasets/               # Additional datasets
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

## ğŸ§ª Testing the Pipeline

1. **API Health**: Visit `http://localhost:5000/api/health`
2. **Data Loading**: Check console output for cleaning statistics
3. **Endpoint Testing**: Use browser or curl for API calls
4. **Dashboard**: Verify all visualizations load correctly
5. **Filtering**: Test various filter combinations
6. **Outlier Detection**: Try different methods and thresholds

## ğŸ¯ Grading Alignment

### Part 1: Data Cleaning & Database (30 pts)
- âœ… CSV loading with proper column mapping
- âœ… Z-score cleaning with threshold 3.0
- âœ… Reporting: original, removed, remaining counts
- âœ… MongoDB/mongomock storage

### Part 2: Flask REST API (40 pts)
- âœ… Health check endpoint
- âœ… Observations with full filtering support
- âœ… Statistics with percentiles
- âœ… Outliers with IQR and z-score methods
- âœ… JSON responses and error handling

### Part 3: Streamlit Client (30 pts)
- âœ… Sidebar controls with all filters
- âœ… Data table with pagination
- âœ… 3+ Plotly visualizations
- âœ… Statistics panel
- âœ… Interactive outlier detection

## ğŸš€ Future Enhancements

- **Geographic Filters**: Bounding box queries
- **Time Series**: Resampling and aggregation
- **Real-time Updates**: WebSocket integration
- **Authentication**: API key management
- **Caching**: Redis for performance
- **Docker**: Containerized deployment

## ğŸ“ Support

For questions or issues:
- Check API health endpoint first
- Verify all dependencies installed
- Ensure both API and client are running
- Check console logs for error messages

---
*Water Quality Data Service - ETL Pipeline with Interactive Visualization*
